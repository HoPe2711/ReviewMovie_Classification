{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "review_hotel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIVYeIitzWyS",
        "outputId": "919a24f8-a180-4cfa-d2c4-17955d563554"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WaHJVPG0jBM",
        "outputId": "54fe3eae-292e-4abe-dda5-6ef986aa1199"
      },
      "source": [
        "!head \"data_vn.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__label__tot Tỷ lệ tốt Giá / Dịch vụ.Lời khuyên tốt cho Vườn quốc gia và Làng. Tốt\r\n",
            "__label__trung_binh Trang thiết bị vệ sinh hơi cũ. Vệ sinh sạch sẽ, nhân viên thân thiện, địa điểm tốt..  Một khách sạn sạch sẽ, thoải mái, dễ chịu, địa điểm tốt\r\n",
            "__label__xuat_sac Nhân viên thân thiện.Hữu ích để đặt tour du lịch.Phòng sạch sẽ và yên tĩnh..Vị trí hoàn hảo cho hang động công viên quốc gia Ke Bang\r\n",
            "__label__kem Quá ồn ào . Thất vọng\r\n",
            "__label__tot Nơi tốt đẹp để có được xung quanh nhân viên thực sự chăm sóc bạn.Ở lại hoàn hảo, siêu nhân viên\r\n",
            "__label__xuat_sac - Phòng tốt / Giường - Nhân viên rất đẹp và hữu ích - Tuyệt vời - Không khí thân thiện.Một nơi đáng yêu để ở lại\r\n",
            "__label__tot Giường khó nhất Ive ngủ vào bữa sáng giá trị nhân viên hữu ích.Giá trị tốt\r\n",
            "__label__xuat_sac Nơi rất đẹp!Gia đình rất thân thiện!.Ở lại rất tốt đẹp\r\n",
            "__label__xuat_sac Todco Pertact.Chúng tôi đã được hai ngày và nó đã hoàn hảo.Họ đã giúp chúng tôi trong mọi thứ chúng tôi có nhu cầu\r\n",
            "__label__xuat_sac Bãi biển đẹp . Một kì nghỉ thoải mái\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKjW08A2-vHk",
        "outputId": "e745e604-e1f8-472f-c666-92ca0d666346"
      },
      "source": [
        "!pip install underthesea"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting underthesea\n",
            "  Downloading underthesea-1.3.3-py3-none-any.whl (7.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 14.4 MB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.2.5)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.10.0+cu111)\n",
            "Collecting transformers>=3.5.0\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea) (2.23.0)\n",
            "Collecting python-crfsuite>=0.9.6\n",
            "  Downloading python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 51.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea) (4.62.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.0.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.13)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 69.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->underthesea) (3.10.0.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 37.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->underthesea) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->underthesea) (3.4.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->underthesea) (4.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->underthesea) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->underthesea) (2019.12.20)\n",
            "Collecting PyYAML\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 53.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=3.5.0->underthesea) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.5.0->underthesea) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=c03532eaf4f6ac0a1627bf95a5087aca5b743d6d3dcdeebd6cb6b6cfb365137f\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: PyYAML, tokenizers, sacremoses, huggingface-hub, unidecode, transformers, seqeval, python-crfsuite, underthesea\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 huggingface-hub-0.1.2 python-crfsuite-0.9.7 sacremoses-0.0.46 seqeval-1.2.2 tokenizers-0.10.3 transformers-4.12.5 underthesea-1.3.3 unidecode-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXxJDNlx2EIP"
      },
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import regex\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "from underthesea import word_tokenize\n",
        "\n",
        "VN_DATA_DIRECT = 'data_vn.txt'\n",
        "\n",
        "# with open(f\"data/stop_word_vn.txt\", encoding=\"utf8\") as f:\n",
        "#     stop_word = f.read().splitlines()\n",
        "\n",
        "\n",
        "def remove_character_not_ness(document):\n",
        "    document = regex.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]', ' ', document)\n",
        "    document = regex.sub(r'[^\\D]', ' ', document)\n",
        "    document = regex.sub(r'\\s+', ' ', document).strip()\n",
        "    return document\n",
        "\n",
        "\n",
        "def no_accent_vietnamese(s):\n",
        "    s = regex.sub(r'[àáạảãâầấậẩẫăằắặẳẵ]', 'a', s)\n",
        "    s = regex.sub(r'[ÀÁẠẢÃĂẰẮẶẲẴÂẦẤẬẨẪ]', 'A', s)\n",
        "    s = regex.sub(r'[èéẹẻẽêềếệểễ]', 'e', s)\n",
        "    s = regex.sub(r'[ÈÉẸẺẼÊỀẾỆỂỄ]', 'E', s)\n",
        "    s = regex.sub(r'[òóọỏõôồốộổỗơờớợởỡ]', 'o', s)\n",
        "    s = regex.sub(r'[ÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠ]', 'O', s)\n",
        "    s = regex.sub(r'[ìíịỉĩ]', 'i', s)\n",
        "    s = regex.sub(r'[ÌÍỊỈĨ]', 'I', s)\n",
        "    s = regex.sub(r'[ùúụủũưừứựửữ]', 'u', s)\n",
        "    s = regex.sub(r'[ƯỪỨỰỬỮÙÚỤỦŨ]', 'U', s)\n",
        "    s = regex.sub(r'[ỳýỵỷỹ]', 'y', s)\n",
        "    s = regex.sub(r'[ỲÝỴỶỸ]', 'Y', s)\n",
        "    s = regex.sub(r'[Đ]', 'D', s)\n",
        "    s = regex.sub(r'[đ]', 'd', s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def remove_stopwords(document):\n",
        "    words = []\n",
        "    for word in document.strip().split():\n",
        "        if word not in stop_word:\n",
        "            words.append(word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "def text_preprocess(document):\n",
        "    document = document.lower()\n",
        "    document = word_tokenize(document, format=\"text\")\n",
        "    document = remove_character_not_ness(document)\n",
        "    #document = remove_stopwords(document)\n",
        "    return document\n",
        "\n",
        "\n",
        "text = []\n",
        "label = []\n",
        "\n",
        "\n",
        "def load_training_data(\n",
        "        vn_directory\n",
        "):\n",
        "    with open(f\"{vn_directory}\", encoding=\"utf8\") as f:\n",
        "        data_set = f.read().splitlines()\n",
        "        for data in data_set:\n",
        "            document = data\n",
        "            tmp = document.split(\" \", 1)\n",
        "            tmp[1] = text_preprocess(tmp[1])\n",
        "            label.append(tmp[0])\n",
        "            text.append(tmp[1])\n",
        "            s = no_accent_vietnamese(tmp[1]).replace('_', ' ')\n",
        "            if tmp[1] != s:\n",
        "                label.append(tmp[0])\n",
        "                text.append(s)\n",
        "\n",
        "\n",
        "load_training_data(VN_DATA_DIRECT)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=0.1, random_state=42, shuffle=True)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train)\n",
        "y_train = label_encoder.transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbmsiqLKD6Nf",
        "outputId": "a733c529-ccc0-4f3c-a1de-d0c6048790c0"
      },
      "source": [
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 6),\n",
        "                                              max_df=0.8,\n",
        "                                              )),\n",
        "                     ('tfidf', TfidfTransformer(use_idf=False, sublinear_tf=True)),\n",
        "                     ('clf', LinearSVC())\n",
        "                     ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training SVM in', train_time, 'seconds.')\n",
        "\n",
        "y_pred = text_clf.predict(X_test)\n",
        "print('SVM, Accuracy =', np.mean(y_pred == y_test))\n",
        "\n",
        "y_pred = text_clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=list(label_encoder.classes_)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training SVM in 61.73260760307312 seconds.\n",
            "SVM, Accuracy = 0.6961386573058359\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "       __label__kem       0.61      0.46      0.52       214\n",
            "   __label__rat_kem       1.00      0.19      0.32        48\n",
            "       __label__tot       0.68      0.82      0.74      2072\n",
            "__label__trung_binh       0.70      0.54      0.61       951\n",
            "  __label__xuat_sac       0.74      0.67      0.70      1273\n",
            "\n",
            "           accuracy                           0.70      4558\n",
            "          macro avg       0.75      0.54      0.58      4558\n",
            "       weighted avg       0.70      0.70      0.69      4558\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OelHX8a6D9Ns",
        "outputId": "22da0eab-6159-40b4-cbae-e784fa46d527"
      },
      "source": [
        "\n",
        "test = ['Bách béo bị đói ăn', 'tàm tạm', 'Tốt', 'thất vỌng', 'tệ', 'Phong kha là dep.', 'Tôi thực sự ấn tượng '\n",
        "                                                                               'với phong cách làm '\n",
        "                                                                               'việc của Chudu24. '\n",
        "                                                                               'Nhiệt tình, '\n",
        "                                                                               'hiệu quả. Tôi cũng '\n",
        "                                                                               'gửi lời cám ơn tới '\n",
        "                                                                               'bộ phận chăm sóc '\n",
        "                                                                               'khách hàng Chudu24 '\n",
        "                                                                               'nói chung và bạn Lai '\n",
        "                                                                               'NV chăm sóc khách '\n",
        "                                                                               'hàng nói riêng. Chúc '\n",
        "                                                                               'Quý Công ty Thành '\n",
        "                                                                               'công hơn nữa trong '\n",
        "                                                                               'tương lai. . Cám ơn '\n",
        "                                                                               'Chudu24']\n",
        "for i in range(len(test)):\n",
        "    test[i] = text_preprocess(test[i])\n",
        "label = text_clf.predict(test)\n",
        "print('Predict label:', list(label_encoder.inverse_transform(label)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict label: ['__label__trung_binh', '__label__trung_binh', '__label__tot', '__label__kem', '__label__rat_kem', '__label__tot', '__label__tot']\n"
          ]
        }
      ]
    }
  ]
}